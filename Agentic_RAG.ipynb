{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "260f69d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from crewai_tools import SerperDevTool, ScrapeWebsiteTool\n",
    "from crewai import Agent, Task, Crew,LLM\n",
    "\n",
    "# Let's load environment variables.\n",
    "load_dotenv()\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "SERPER_API_KEY = os.getenv(\"SERPER_API_KEY\")\n",
    "\n",
    "# verification of API's\n",
    "\n",
    "if not GROQ_API_KEY or not SERPER_API_KEY :\n",
    "    raise ValueError (\"Set necessary API keys in env file.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161da0f5",
   "metadata": {},
   "source": [
    "os - in order to access env variables and system config\n",
    "dotenv - for loading API_keys\n",
    "\n",
    "FAISS - Stores and searches embeddings in order to retrieve the most relevant text chunks\n",
    "PyPDFLoader - Read's PDF's and convert them into text\n",
    "RecursiveCharacterTextSplitter - Split long documents into smaller, chunks holding meaning\n",
    "\n",
    "HuggingFaceEmbeddings - convert text chunks into numberical embeddings (vectors)\n",
    "chatGroq - Connects langchain to Groq-hosted LLM's for faster inference.\n",
    "\n",
    "SerperDevTool - For search in internet.\n",
    "ScrapeWebsiteTool - Extract raw content from webpages.\n",
    "\n",
    "Agent - helps to define an AI role with goals,tools and behaviour\n",
    "Task - Describe a specific job an agent must complete\n",
    "Crew - Orchestrate multiple agents to work together on tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e797bb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LiteLLM is not available, falling back to LiteLLM\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Fallback to LiteLLM is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      3\u001b[39m llm = ChatGroq(\n\u001b[32m      4\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mllama-3.3-70b-versatile\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     temperature=\u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     max_retries=\u001b[32m2\u001b[39m,\n\u001b[32m      9\u001b[39m )\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m#Intialize LLM for CrewAI agents\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m crew_llm = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemini/gemini-2.0-flash\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mGEMINI_API_KEY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_litellm\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\devan\\Desktop\\Agentic_RAG\\venv\\Lib\\site-packages\\crewai\\llm.py:423\u001b[39m, in \u001b[36mLLM.__new__\u001b[39m\u001b[34m(cls, model, is_litellm, **kwargs)\u001b[39m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m LITELLM_AVAILABLE:\n\u001b[32m    422\u001b[39m     logger.error(\u001b[33m\"\u001b[39m\u001b[33mLiteLLM is not available, falling back to LiteLLM\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mFallback to LiteLLM is not available\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    425\u001b[39m instance = \u001b[38;5;28mobject\u001b[39m.\u001b[34m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    426\u001b[39m \u001b[38;5;28msuper\u001b[39m(LLM, instance).\u001b[34m__init__\u001b[39m(model=model, is_litellm=\u001b[38;5;28;01mTrue\u001b[39;00m, **kwargs)\n",
      "\u001b[31mImportError\u001b[39m: Fallback to LiteLLM is not available"
     ]
    }
   ],
   "source": [
    "#Initializing main LLM - for routing and final answer.\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0,\n",
    "    max_tokens=500,\n",
    "    groq_api_key=GROQ_API_KEY,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "#Intialize LLM for CrewAI agents\n",
    "\n",
    "crew_llm = LLM(\n",
    "    model=\"gemini/gemini-2.0-flash\",\n",
    "    api_key=GEMINI_API_KEY,\n",
    "    max_tokens=500,\n",
    "    temperature=0.7,\n",
    "    is_litellm=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d0ea92",
   "metadata": {},
   "source": [
    "We use the main LLM for routing decisions,final asnwers, and grounding on retireved context - so we keep temperature = 0\n",
    "\n",
    "so it can be - consistent,repeatable and predictable.\n",
    "\n",
    "We use crew_llm_model for search query formulation,Summarization,exploration, taking decision on how to approach a task - so we increase the temperature\n",
    "\n",
    "so it can be - flexible,mild creative, ambigue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01567217",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The decision maker - check if we can answer from local knowledge or  need to search on the web\n",
    "\n",
    "def check_local_knowledge(query,context):\n",
    "    \"\"\"\n",
    "    So this router function will determine whether we can answer from local knowledge or not.\n",
    "    It returns True if local context is sufficient, False otherwise.\n",
    "    \n",
    "    \"\"\"\n",
    "    prompt = ''' Role: Question-Answering Assistant\n",
    "\n",
    "    Task: Determine whether the system can answer the user's question based on the provided text.\n",
    "\n",
    "    Instructions:\n",
    "    - Analyze the text and identify if it contains the necessary information to answer the user's question.\n",
    "    - Provide a clear and concise response indicating whether the system can answer the question or not.\n",
    "    - Your response should include only a single word: \"Yes\" or \"No\". Noting else.\n",
    "\n",
    "    Output Format:\n",
    "    - Answer: Yes/No\n",
    "\n",
    "    Examples:\n",
    "    Input:\n",
    "        Text: There is 14 districts in Kerala.\n",
    "        User Queestion: How many districts are there in Kerala ?\n",
    "    Expected Output:\n",
    "        Answer: Yes\n",
    "    \n",
    "    Input:\n",
    "        Text: Peacock is the national bird of India.\n",
    "        User Question: What is the national bird of China?\n",
    "    Expected Output:\n",
    "        Answer: No\n",
    "    \n",
    "    Now analyze this:\n",
    "    Input:\n",
    "        User Question: {query}\n",
    "        Text: {text}\n",
    "    Output:'''\n",
    "\n",
    "    formatted_prompt = prompt.format(text=context, query=query)\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "\n",
    "    # We will extract the response.\n",
    "\n",
    "    answer = response.content.strip().lower()\n",
    "    return 'Yes' in answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccb234b",
   "metadata": {},
   "source": [
    "We will use this function in order to understand whether we could fetch the information from the documents we provided itself or we should do Internet search.\n",
    "\n",
    "The query = user question and the context = document data is passed into the function.\n",
    "\n",
    "Prompt = We will describe and let the LLM know what it's role and task along with examples for ensuring the structure of output\n",
    "\n",
    "format = we inject the query and context into the prompt\n",
    "\n",
    "invoke = we use LLM for output answer - which should be Yes or No\n",
    "\n",
    "will return True or False based on the response. (LLM's are mainly trained for give Yes or No answers over True or False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfebec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web searching and Scraping Agents\n",
    "\n",
    "def setup_web_scraping_crew():\n",
    "    \"\"\"\n",
    "    It will setup the web scraping crew with search and scraping agents.\n",
    "    It will also return a configured crew  - which is ready to fetch web content.\n",
    "\n",
    "    \"\"\"\n",
    "    #Initialize tools\n",
    "    search_tool = SerperDevTool()\n",
    "    scrape_tool = ScrapeWebsiteTool()\n",
    "\n",
    "    # Define the Websearch Agent.\n",
    "\n",
    "    web_search_agent = Agent(\n",
    "        role=\"Expert Web Search Agent\",\n",
    "        goal=\"Identify and retrieve relevant web pages for user queries\",\n",
    "        backstory=\"An expert in identifying valuable web sources using search engines.\",\n",
    "        tools=[search_tool],\n",
    "        verbose=True,\n",
    "        llm=crew_llm_model\n",
    "    )\n",
    "\n",
    "    # Define the Webscraping Agent.\n",
    "\n",
    "    web_scraper_agent = Agent(\n",
    "        role=\"Expert Web Scraper Agent\",\n",
    "        goal=\"Extract and analyze content from web pages\",\n",
    "        backstory=\"A highly skilled web scraper who can analyze and summarize website content accurately.\",\n",
    "        tools=[scrape_tool],\n",
    "        verbose=True,\n",
    "        llm=crew_llm_model\n",
    "    )\n",
    "\n",
    "    # Define the search task\n",
    "\n",
    "    search_task = Task(\n",
    "        description=(\n",
    "            \"Search for the most relevant and authorative web page about: '{topic}'.\"\n",
    "            \"Find credible sources that contain detailed, accurate information.\"\n",
    "            \"Return the URL and a brief description of why this source is relevant.\"\n",
    "        ),\n",
    "        expected_output=(\n",
    "            \"The URL of the most relevant web page for '{topic}' with a breif explanation \"\n",
    "            \"of its relevance and key topics covered.\"   \n",
    "        ),\n",
    "        agent=web_search_agent,\n",
    "    )\n",
    "\n",
    "\n",
    "    scraping_task = Task(\n",
    "        description=(\n",
    "            \"Extract and analyze the main content from the web page found for '{topic}'.\"\n",
    "            \"Focus on factual information, key concepts, and important details.\"\n",
    "            \"Summarize the findings in a clear, structured manner.\"\n",
    "        ),\n",
    "        expected_output=(\n",
    "            \"A comprehensive summary of the web page content related to '{topic}',\"\n",
    "            \"highlighting key information,facts, and concepts in a well-organized format.\"\n",
    "        ),  \n",
    "        agent=web_scraper_agent,\n",
    "    )\n",
    "\n",
    "    crew = Crew(\n",
    "        agents=[web_search_agent,web_scraper_agent],\n",
    "        tasks=[search_task,scraping_task],\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    return crew\n",
    "\n",
    "def get_web_content(query):\n",
    "    \"\"\"\n",
    "    By using the scraping crew get the content from web.\n",
    "    On basis of the query return a summarized version of web content.\n",
    "    \n",
    "    \"\"\"\n",
    "    print(f\"Searching the web for:{query}\")\n",
    "    crew = setup_web_scraping_crew()\n",
    "    result = crew.kickoff(inputs={\"topic\":query})\n",
    "    return result.raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28030505",
   "metadata": {},
   "source": [
    "- We initializes the tools into search_tool and scrape_tool.\n",
    "- searchtool - to search like google\n",
    "- scrapetool - to extract the content\n",
    "- They are called Retrieval primitives.\n",
    "\n",
    "**Web Search Agent or Web Scraper Agent**\n",
    "\n",
    "- role - specifies the character of our agent\n",
    "- goal - The success criteria the agent optimizes for while reasoning\n",
    "- backstory - Giving extra behavioural context\n",
    "- tools - the external function that the agent is allowed to call\n",
    "- verbose - (True) it will logs (or print out) agent's internal reasoning steps, tool calls, and task progress to the console\n",
    "- llm - the llm instance for output generation.\n",
    "\n",
    "**Searching and Scraping task**\n",
    "\n",
    "- description - The instrucitons an agent follows to perform the task.\n",
    "- expected_output - tells how the output should be\n",
    "- agent - the agent responsible for this task\n",
    "\n",
    "Nb -: we can write those description and extra_output instructions in single line, we does this way for extra readability (better not miss spaces after one line)\n",
    "\n",
    "**Crew that cordinate**\n",
    "\n",
    "crew - control the coordination of agents and tasks\n",
    "- agents - set of agent available to execute task\n",
    "- tasks - The ordered list of tasks the crew will run.\n",
    "- verbos - print execution flow,agent decisions,and tool usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aed0e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating vector database\n",
    "# Stores local PDF knowledge.\n",
    "\n",
    "def setup_vector_db(pdf_path):\n",
    "    \"\"\"\n",
    "    setup vector database from PDF.\n",
    "\n",
    "    steps:\n",
    "    1. Load PDF and extract text\n",
    "    2. Split text into manageable chunks\n",
    "    3. Create embeddings for each chunk\n",
    "    4. Store in FAISS vector database for fast retrieval\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Loading PDF from: {pdf_path}\")\n",
    "\n",
    "    #Load and extract PDF content\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000, #Each chunk with around 1000 characters.\n",
    "        chunk_overlap=50, #50 characters overlap to maintain context\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split into {len(chunks)} chunks\")\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "    )\n",
    "\n",
    "    vector_db = FAISS.from_documents(chunks,embeddings)\n",
    "    print(\"Vector database created successfully\")\n",
    "\n",
    "    return vector_db\n",
    "\n",
    "def get_local_content(vector_db, query):\n",
    "    \"\"\"\n",
    "    Retrieve relevant content from vector database.\n",
    "\n",
    "    Uses similarity search to find the 5 most relevant chunks for the given query.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    docs = vector_db.similarity_search(query, k=5)\n",
    "    # combine the top 5 most relevant chunks\n",
    "\n",
    "    context = \" \".join([doc.page_content for doc in docs])\n",
    "    return context\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f601f7cb",
   "metadata": {},
   "source": [
    "- PDF loaded for extraction\n",
    "- Using RecusiveCharacterTextSplitter from langchain_text_splitters we split the content into chunks\n",
    "- Converts each chunk into a vector(numbers) , similar meanings have vectors which are closer - allows the model to be semantic based over keyword- based\n",
    "- For fast similarity search -> store the embeddings into FAISS\n",
    "- similarity_search allows find the most important chunks that suites for the query (semantic matching)\n",
    "\n",
    "There are many embedding systems like BERT,OpenAI embeddings,etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25eb07fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_answer(context,query):\n",
    "    \"\"\"\n",
    "    We will generate the final answer using the LLM along with the retrieved context.\n",
    "\n",
    "    Combines the context and user query into a structured prompt \n",
    "    that instructs the LLM to answer based on the provided inofrmation.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful asssistant. Use the provided context to answer the user's question accurately.\"\n",
    "            \"If the context doesn't contain enough information, say so clearly.\"\n",
    "\n",
    "        ),\n",
    "\n",
    "        (\"system\", f\"Context: {context}\"),\n",
    "        (\"human\",query),\n",
    "    ]\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d34dc2",
   "metadata": {},
   "source": [
    "Once we have the necessary context (from either local or web), we can generate the final answer by passing the context and query to aan LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43a6f72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main query processing Pipelin.\n",
    "\n",
    "def process_query(query, vector_db, local_context):\n",
    "\n",
    "    \"\"\"\n",
    "    Main function for process the user query through the agentic RAG pipeline.\n",
    "\n",
    "    Flow:\n",
    "    1. check if we can answer from local knowledge (routing)\n",
    "    2. If Yes: Retrieve from vector database.\n",
    "    3. If No: Search and Scrape the web\n",
    "    4. Generate final anwser using retrieved context.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Processing query -: {query}\")\n",
    "    \n",
    "    # Check if answering from local context itself is possible or not.\n",
    "\n",
    "    can_answer_locally = check_local_knowledge(query, local_context)\n",
    "    print(f\"\\nCan answer from local knowledge ? -: {can_answer_locally} \")\n",
    "\n",
    "    if can_answer_locally:\n",
    "        print(\"! Retrieving from local documents..\")\n",
    "        context = get_local_content(vector_db,query)\n",
    "        source = \"LOCAL DOCUMENTS\"\n",
    "    else:\n",
    "        print(\"! Searching the web ..\")\n",
    "        context = get_web_content(query)\n",
    "        source = \"WEB SEARCH\"\n",
    "\n",
    "    print(f\"\\nRetrieved context from {source}.\")\n",
    "    print(f\"Context length: {len(context)} characters\\n\")\n",
    "\n",
    "    print(\"-> Generating Final answer...!\\n\")\n",
    "    answer = generate_final_answer(context,query)\n",
    "    \n",
    "    return answer, source\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b275adcb",
   "metadata": {},
   "source": [
    "- Routing : Decide between local or web retrieval\n",
    "- Retrieval : Get relevant context from chosen source\n",
    "- Generation : Create answer using context and query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "627d76b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Agentic RAG system..\n",
      "\n",
      "Step 1: Setting up vector database..\n",
      "Loading PDF from: C:\\Users\\devan\\Desktop\\Agentic_RAG\\Basic-Biology-an-introduction.pdf\n",
      "Split into 127 chunks\n",
      "Vector database created successfully\n",
      "Step 2: Loading initial context for routing..\n",
      "Local context ready (3506 characters)\n",
      "\n",
      "Processing query -: What is Agentic RAG?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error instantiating LLM from unknown object type: Error importing native provider: OPENAI_API_KEY is required\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Can answer from local knowledge ? -: False \n",
      "! Searching the web ..\n",
      "Searching the web for:What is Agentic RAG?\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Error importing native provider: OPENAI_API_KEY is required",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\devan\\Desktop\\Agentic_RAG\\venv\\Lib\\site-packages\\crewai\\llm.py:413\u001b[39m, in \u001b[36mLLM.__new__\u001b[39m\u001b[34m(cls, model, is_litellm, **kwargs)\u001b[39m\n\u001b[32m    410\u001b[39m     kwargs_copy = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k != \u001b[33m\"\u001b[39m\u001b[33mprovider\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m    411\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    412\u001b[39m         Self,\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m         \u001b[43mnative_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_copy\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    414\u001b[39m     )\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\devan\\Desktop\\Agentic_RAG\\venv\\Lib\\site-packages\\crewai\\llms\\providers\\openai\\completion.py:250\u001b[39m, in \u001b[36mOpenAICompletion.__init__\u001b[39m\u001b[34m(self, model, api_key, base_url, organization, project, timeout, max_retries, default_headers, default_query, client_params, temperature, top_p, frequency_penalty, presence_penalty, max_tokens, max_completion_tokens, seed, stream, response_format, logprobs, top_logprobs, reasoning_effort, provider, interceptor, api, instructions, store, previous_response_id, include, builtin_tools, parse_tool_outputs, auto_chain, auto_chain_reasoning, **kwargs)\u001b[39m\n\u001b[32m    240\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m    241\u001b[39m     model=model,\n\u001b[32m    242\u001b[39m     temperature=temperature,\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     **kwargs,\n\u001b[32m    248\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m client_config = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_client_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.interceptor:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\devan\\Desktop\\Agentic_RAG\\venv\\Lib\\site-packages\\crewai\\llms\\providers\\openai\\completion.py:338\u001b[39m, in \u001b[36mOpenAICompletion._get_client_params\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY is required\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    340\u001b[39m base_params = {\n\u001b[32m    341\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mapi_key\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.api_key,\n\u001b[32m    342\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33morganization\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.organization,\n\u001b[32m   (...)\u001b[39m\u001b[32m    351\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdefault_query\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.default_query,\n\u001b[32m    352\u001b[39m }\n",
      "\u001b[31mValueError\u001b[39m: OPENAI_API_KEY is required",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     35\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     22\u001b[39m queries = [\n\u001b[32m     23\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWhat is Agentic RAG?\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;66;03m#To trigger web search\u001b[39;00m\n\u001b[32m     24\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWhat are the key principles discussed in the document?\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;66;03m#To use the document data\u001b[39;00m\n\u001b[32m     25\u001b[39m \n\u001b[32m     26\u001b[39m ]\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries:\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     answer, source = \u001b[43mprocess_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvector_db\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlocal_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFINAL ANSWER (source : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m): \u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mprocess_query\u001b[39m\u001b[34m(query, vector_db, local_context)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m! Searching the web ..\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     context = \u001b[43mget_web_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m     source = \u001b[33m\"\u001b[39m\u001b[33mWEB SEARCH\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRetrieved context from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 79\u001b[39m, in \u001b[36mget_web_content\u001b[39m\u001b[34m(query)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[33;03mBy using the scraping crew get the content from web.\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[33;03mOn basis of the query return a summarized version of web content.\u001b[39;00m\n\u001b[32m     76\u001b[39m \n\u001b[32m     77\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSearching the web for:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m crew = \u001b[43msetup_web_scraping_crew\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m result = crew.kickoff(inputs={\u001b[33m\"\u001b[39m\u001b[33mtopic\u001b[39m\u001b[33m\"\u001b[39m:query})\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result.raw\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36msetup_web_scraping_crew\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     11\u001b[39m scrape_tool = ScrapeWebsiteTool()\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Define the Websearch Agent.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m web_search_agent = \u001b[43mAgent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrole\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mExpert Web Search Agent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgoal\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mIdentify and retrieve relevant web pages for user queries\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackstory\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAn expert in identifying valuable web sources using search engines.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43msearch_tool\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcrew_llm_model\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Define the Webscraping Agent.\u001b[39;00m\n\u001b[32m     26\u001b[39m web_scraper_agent = Agent(\n\u001b[32m     27\u001b[39m     role=\u001b[33m\"\u001b[39m\u001b[33mExpert Web Scraper Agent\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     28\u001b[39m     goal=\u001b[33m\"\u001b[39m\u001b[33mExtract and analyze content from web pages\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     llm=crew_llm_model\n\u001b[32m     33\u001b[39m )\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\devan\\Desktop\\Agentic_RAG\\venv\\Lib\\site-packages\\crewai\\agent\\internal\\meta.py:57\u001b[39m, in \u001b[36mAgentMeta.__new__.<locals>.post_init_setup_with_extensions\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost_init_setup_with_extensions\u001b[39m(\u001b[38;5;28mself\u001b[39m: Any) -> Any:\n\u001b[32m     49\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Wrap post_init_setup to apply extensions after initialization.\u001b[39;00m\n\u001b[32m     50\u001b[39m \n\u001b[32m     51\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     55\u001b[39m \u001b[33;03m        The agent instance\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     result = \u001b[43moriginal_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m     a2a_value = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33ma2a\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m a2a_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\devan\\Desktop\\Agentic_RAG\\venv\\Lib\\site-packages\\crewai\\agent\\core.py:274\u001b[39m, in \u001b[36mAgent.post_init_setup\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[38;5;129m@model_validator\u001b[39m(mode=\u001b[33m\"\u001b[39m\u001b[33mafter\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost_init_setup\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Self:\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m     \u001b[38;5;28mself\u001b[39m.llm = \u001b[43mcreate_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_calling_llm \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    276\u001b[39m         \u001b[38;5;28mself\u001b[39m.function_calling_llm, BaseLLM\n\u001b[32m    277\u001b[39m     ):\n\u001b[32m    278\u001b[39m         \u001b[38;5;28mself\u001b[39m.function_calling_llm = create_llm(\u001b[38;5;28mself\u001b[39m.function_calling_llm)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\devan\\Desktop\\Agentic_RAG\\venv\\Lib\\site-packages\\crewai\\utilities\\llm_utils.py:66\u001b[39m, in \u001b[36mcreate_llm\u001b[39m\u001b[34m(llm_value)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     65\u001b[39m     logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError instantiating LLM from unknown object type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\devan\\Desktop\\Agentic_RAG\\venv\\Lib\\site-packages\\crewai\\utilities\\llm_utils.py:53\u001b[39m, in \u001b[36mcreate_llm\u001b[39m\u001b[34m(llm_value)\u001b[39m\n\u001b[32m     50\u001b[39m     base_url: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28mgetattr\u001b[39m(llm_value, \u001b[33m\"\u001b[39m\u001b[33mbase_url\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     51\u001b[39m     api_base: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28mgetattr\u001b[39m(llm_value, \u001b[33m\"\u001b[39m\u001b[33mapi_base\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     65\u001b[39m     logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError instantiating LLM from unknown object type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\devan\\Desktop\\Agentic_RAG\\venv\\Lib\\site-packages\\crewai\\llm.py:418\u001b[39m, in \u001b[36mLLM.__new__\u001b[39m\u001b[34m(cls, model, is_litellm, **kwargs)\u001b[39m\n\u001b[32m    416\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m418\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError importing native provider: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    420\u001b[39m \u001b[38;5;66;03m# FALLBACK to LiteLLM\u001b[39;00m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m LITELLM_AVAILABLE:\n",
      "\u001b[31mImportError\u001b[39m: Error importing native provider: OPENAI_API_KEY is required"
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "from pathlib import Path\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the Agentic RAG system.\n",
    "\n",
    "    \"\"\"\n",
    "    pdf_path = r\"C:\\Users\\devan\\Desktop\\Agentic_RAG\\Basic-Biology-an-introduction.pdf\"\n",
    "    # pdf_path = Path(input(\"Enter the pdf path: \").strip()) #just to input the path\n",
    "\n",
    "    print(\"Initializing Agentic RAG system..\\n\")\n",
    "\n",
    "    # Initialize vector database\n",
    "    print(\"Step 1: Setting up vector database..\")\n",
    "    vector_db = setup_vector_db(pdf_path)\n",
    "\n",
    "    # Get initial context for routing decisions\n",
    "    print(\"Step 2: Loading initial context for routing..\")\n",
    "    local_context = get_local_content(vector_db,\"general overview\")\n",
    "    print(f\"Local context ready ({len(local_context)} characters)\\n\")\n",
    "\n",
    "    queries = [\n",
    "        \"What is Agentic RAG?\", #To trigger web search\n",
    "        \"What are the key principles discussed in the document?\", #To use the document data\n",
    "\n",
    "    ]\n",
    "\n",
    "    for query in queries:\n",
    "        answer, source = process_query(query,vector_db,local_context)\n",
    "        \n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"FINAL ANSWER (source : {source}): \")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"\\n{answer}\\n\")\n",
    "        print(f\"{'='*60}\\n\\n\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10531da3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1d5dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
